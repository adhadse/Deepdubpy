import numpy as np
import pandas as pd
from pathlib import Path
from moviepy.editor import AudioFileClip
from moviepy.config import get_setting
from moviepy.tools import subprocess_call

from spleeter.separator import Separator
from spleeter.audio import Codec


class DeepdubAudio():
  def __init__(self, project_name, sentence_df, audio_path):
    """
    Manages audio for Deepdub.
    - `audio_df` will contains information required to clip audio,
      including parts where sentences are spoken and where they are not.
    ### Parameters:
    - `project_name`: name for which you want to deepdub
    - `sentence_df`: a `DeepdubSentences.get_sentences()` instance
    - `audio_path`: path to extracted audio of clipped video
    """
    self.sentence_df = sentence_df.reset_index().set_index(["hash"])
    self.audio_path = audio_path
    self.AUDIO_OUTPUT_DIR = f'./output_dir/{project_name}/audio_segments'

    # `audio_df` requires ranges where sentences are spoken and also where
    # they are accompaniment; in whole `audio_path` file.
    # sentence_df only has info where they sentences are spoken,
    # we need to add range were accompaniment exist.
    vals = self.sentence_df[['start', 'end']].stack().unique()
    vals2 = np.concatenate([np.array([0], dtype=vals.dtype), vals])
    self.audio_df = pd.DataFrame(zip(vals2, vals), columns=['start', 'end'])

    # but this dataframe; `audio_df` doesn't have information about
    # last audio segment after last sentence is spoken.
    self.audio_df = pd.concat([
      self.audio_df, pd.DataFrame({
        "start": [self.audio_df.iloc[-1].end],
        "end": [pd.to_datetime(str(
          AudioFileClip(self.audio_path).duration), format="%S.%f")]
      })], ignore_index=True, axis=0)
    
    # Add `hash` of `start` and `end` timestamp to uniquely identify
    # Add `path` so that we can identify when concatencating audio segments 
    # which are accompaniments and which are generated by TTS model.
    self.audio_df["hash"] = pd.util.hash_pandas_object(self.audio_df[["start", "end"]], index=False)
    self.audio_df[["path"]] = self.audio_df[["hash"]].applymap(
      lambda h: str(h) + "_gen.wav" if h in self.sentence_df.index else str(h) + ".wav")

  def create_audio_segments(self):
    """Create audio segments based on `audio_df`"""
    Path(self.AUDIO_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)

    for index, row in self.audio_df.iterrows():
      audio_clip_path = f'{self.AUDIO_OUTPUT_DIR}/{row.hash}.wav'
      self.segment_audio(self.audio_path,
                         self.__timestamp_to_seconds(row["start"]),
                         self.__timestamp_to_seconds(row["end"]),
                         audio_clip_path)

    # Save paths of segments to `audio_segments_list.txt`
    audio_segments_list = f'{self.AUDIO_OUTPUT_DIR}/audio_segments_list.txt'
    with open(audio_segments_list, 'w') as fout:
      for i, row in self.audio_df.iterrows():
        fout.write(f'file {row["path"]}\n')

  def segment_audio(self, input_audio, clip_from, clip_to, output, acodec="pcm_s16le"):
    """Clip audio from another audio file and save it in `output`"""
    cmd = [get_setting("FFMPEG_BINARY"), "-y",
           "-ss", "%0.3f" % clip_from,
           "-i", input_audio,
           "-t", "%0.3f" % (clip_to-clip_from),
           "-acodec", acodec,
           output]
    subprocess_call(cmd)

  def concatenate_generated_audio_segments(self):
    """Concatenate audio segments using `audio_segments_list.txt`
    to generate `audio_gen.wav`
    ### Returns:
    - `generated_audio_path`
    """
    generated_audio_path = f'{self.AUDIO_OUTPUT_DIR}/audio_gen.wav'
    cmd = [get_setting("FFMPEG_BINARY"), "-y",
           "-f", "concat",
           "-safe", "0",
           "-i", f'{self.AUDIO_OUTPUT_DIR}/audio_segments_list.txt',
           "-c", "copy", 
           generated_audio_path]
    subprocess_call(cmd)
    return generated_audio_path
  
  def extract_vocal_and_accompaniments(self):
    """Generates vocals and accompaniments
    using `sentence_df`; generating only for audio where sentence exist"""
    separator = Separator('spleeter:2stems')
    for sentence_audio in (
      f'{self.AUDIO_OUTPUT_DIR}/{str(row.hash)}.wav'
      for i, row in self.sentence_df.reset_index().iterrows()):
      separator.separate_to_file(audio_descriptor=sentence_audio,
                                 destination=self.AUDIO_OUTPUT_DIR,
                                 synchronous=False,
                                 codec=Codec.WAV,
                                 filename_format='{filename}_{instrument}.{codec}')
    separator.join()

  def mix_generated_vocals_and_accompaniments(self):
    """TODO
    """
    pass

  def __timestamp_to_seconds(self, timestamp):
    """Convert Pandas timestamp object to float seconds with decimal
       part representing milliseconds"""
    return timestamp.hour + timestamp.second + timestamp.microsecond/1000000
