# Copyright 2022 Anurag Dhadse. All Rights Reserved.
# This file is part of Deepdubpy.
#
# Deepdubpy is free software: you can redistribute it and/or modify it under the terms of
# the GNU General Public License as published by the Free Software Foundation, either
# version 2 of the License, or (at your option) any later version.
#
# Deepdubpy is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR
# PURPOSE. See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along with Deepdubpy.
# If not, see <https://www.gnu.org/licenses/>.

import numpy as np
import pandas as pd
from pathlib import Path
from moviepy.editor import AudioFileClip
from moviepy.config import get_setting
from moviepy.tools import subprocess_call

from spleeter.separator import Separator
from spleeter.audio import Codec


class DeepdubAudio:
   """Manages audio for Deepdubpy.
     
    This class maintains `audio_df` which contains information required to clip audio,
    including parts where sentences are spoken and where they are not.
    Args:
      project_name: name for which you want to deepdub
      sentence_df: a `DeepdubSentences.get_sentences()` instance
      audio_path: path to extracted audio of clipped video
    """
  def __init__(self, project_name, sentence_df, audio_path):
    self.sentence_df = sentence_df.reset_index().set_index(["hash"])
    self.audio_path = audio_path
    self.AUDIO_OUTPUT_DIR = f'./output_dir/{project_name}/audio_segments'

    # `audio_df` requires ranges where sentences are spoken and also where
    # they are accompaniment; in whole `audio_path` file.
    # sentence_df only has info where they sentences are spoken,
    # we need to add range were accompaniment exist.
    vals = self.sentence_df[['start', 'end']].stack().unique()
    vals2 = np.concatenate([np.array([0], dtype=vals.dtype), vals])
    self.audio_df = pd.DataFrame(zip(vals2, vals), columns=['start', 'end'])

    # but this dataframe; `audio_df` doesn't have information about
    # last audio segment after last sentence is spoken.
    self.audio_df = pd.concat([
      self.audio_df, pd.DataFrame({
        "start": [self.audio_df.iloc[-1].end],
        "end": [pd.to_datetime(
          AudioFileClip(self.audio_path).duration, unit='s')]
      })], ignore_index=True, axis=0)
    
    # Add `hash` of `start` and `end` timestamp to uniquely identify
    # Add `path` so that we can identify when concatencating audio segments 
    # which are accompaniments and which are generated by TTS model.
    self.audio_df["hash"] = pd.util.hash_pandas_object(self.audio_df[["start", "end"]], index=False)
    self.audio_df[["path"]] = self.audio_df[["hash"]].applymap(
      lambda h: str(h) + "_gen.wav" if h in self.sentence_df.index else str(h) + ".wav")

  def create_audio_segments(self):
    """Create audio segments based on `audio_df`"""
    Path(self.AUDIO_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)

    for index, row in self.audio_df.iterrows():
      audio_clip_path = f'{self.AUDIO_OUTPUT_DIR}/{row.hash}.wav'
      self.segment_audio(self.audio_path,
                         self.__timestamp_to_seconds(row["start"]),
                         self.__timestamp_to_seconds(row["end"]),
                         audio_clip_path)

    # Save paths of segments to `audio_segments_list.txt`
    audio_segments_list = f'{self.AUDIO_OUTPUT_DIR}/audio_segments_list.txt'
    with open(audio_segments_list, 'w') as fout:
      for i, row in self.audio_df.iterrows():
        fout.write(f'file {row["path"]}\n')

  def segment_audio(self, input_audio, clip_from, clip_to, output, acodec="pcm_s16le"):
    """Clip audio from another audio file and save it in `output`"""
    cmd = [get_setting("FFMPEG_BINARY"), "-y",
           "-ss", "%0.3f" % clip_from,
           "-i", input_audio,
           "-t", "%0.3f" % (clip_to-clip_from),
           "-acodec", acodec,
           output]
    subprocess_call(cmd)

  def concatenate_generated_audio_segments(self):
    """Concatenate audio segments using `audio_segments_list.txt`
    to generate `audio_gen.wav`
    ### Returns:
    - generated_audio_path
    """
    generated_audio_path = f'{self.AUDIO_OUTPUT_DIR}/audio_gen.wav'
    cmd = [get_setting("FFMPEG_BINARY"), "-y",
           "-f", "concat",
           "-safe", "0",
           "-i", f'{self.AUDIO_OUTPUT_DIR}/audio_segments_list.txt',
           "-c", "copy", 
           generated_audio_path]
    subprocess_call(cmd)
    return generated_audio_path
  
  def extract_vocal_and_accompaniments(self):
    """Generates vocals and accompaniments
    using `sentence_df`; generating only for audio where sentence exist"""
    separator = Separator('spleeter:2stems')
    for sentence_audio in (
      f'{self.AUDIO_OUTPUT_DIR}/{str(row.hash)}.wav'
      for i, row in self.sentence_df.reset_index().iterrows()):
      separator.separate_to_file(audio_descriptor=sentence_audio,
                                 destination=self.AUDIO_OUTPUT_DIR,
                                 synchronous=False,
                                 codec=Codec.WAV,
                                 filename_format='{filename}_{instrument}.{codec}')
    separator.join()

  def mix_generated_vocals_and_accompaniments(self):
    """TODO
    """
    pass

  def __timestamp_to_seconds(self, timestamp):
    """Convert Pandas timestamp object to float seconds with decimal
       part representing milliseconds"""
    return (timestamp.hour * 3600 ) + (
      timestamp.minute * 60) + (
      timestamp.second + timestamp.microsecond/1000000)
